{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "from gensim.models import word2vec\n",
    "import pandas as pd\n",
    "\n",
    "#  wget http://www.nltk.org/nltk_data/packages/tokenizers/punkt.zip\n",
    "# Extract it in ~/nltk_data/packages/tokenizers/\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def description_to_wordlist( description, remove_stopwords=False ):\n",
    "    # Convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "\n",
    "    des_text = BeautifulSoup(description).get_text()\n",
    "    des_text = re.sub(\"[^a-zA-Z]\",\" \", des_text)\n",
    "    words = des_text.lower().split()\n",
    "   \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "  \n",
    "    return(words)\n",
    "\n",
    "def description_to_sentences( description, tokenizer, remove_stopwords=False ):\n",
    "    # Split a paragraph into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    \n",
    "    raw_sentences = tokenizer.tokenize(description.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        \n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append( description_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "            \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def train_w2v(sentences_list, num_features=300, min_word_count=10, \n",
    "              num_workers=4, context=5, downsampling=1e-3):\n",
    "    # num_features = word vector dimensionality                      \n",
    "    # min_word_count = minimum word count                        \n",
    "    # num_workers = number of threads\n",
    "    # context = context window size                                                                                    \n",
    "    # downsampling = downsample frequent words\n",
    "\n",
    "    # Initialize and train the model\n",
    "\n",
    "    print \"Training model...\"\n",
    "    model = word2vec.Word2Vec(sentences_list, workers=num_workers, \\\n",
    "                size=num_features, min_count = min_word_count, \\\n",
    "                window = context, sample = downsampling)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    model_name = \"{}features_{}minwords_{}context\"\\\n",
    "    .format(num_features, min_word_count, context)\n",
    "    model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/bs4/__init__.py:182: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/usr/lib/python2.7/dist-packages/bs4/__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/65497012.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/usr/lib/python2.7/dist-packages/bs4/__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/65516012.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/usr/lib/python2.7/dist-packages/bs4/__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/6552101\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled test set"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/bs4/__init__.py:182: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/usr/lib/python2.7/dist-packages/bs4/__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/65527\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/bs4/__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januarya/14146012.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv').fillna(\"\")\n",
    "test = pd.read_csv('data/test.csv').fillna(\"\")\n",
    "idx = test.id.values.astype(int)\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)\n",
    "y = train.median_relevance.values\n",
    "train = train.drop(['median_relevance', 'relevance_variance'], axis=1)\n",
    "\n",
    "# Extract sentences from descriptions of both training and test sets\n",
    "sentences = []\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for description in train[\"product_description\"]:\n",
    "    sentences += description_to_sentences(description.decode(\"utf8\"), tokenizer)\n",
    "\n",
    "print \"Parsing sentences from unlabeled test set\"\n",
    "for description in test[\"product_description\"]:\n",
    "    sentences += description_to_sentences(description.decode(\"utf8\"), tokenizer)\n",
    "\n",
    "# Train Model    \n",
    "train_w2v(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'thinkpad', 0.8476001620292664),\n",
       " (u'ibm', 0.7556716799736023),\n",
       " (u'intel', 0.6887473464012146),\n",
       " (u'notebook', 0.6659045815467834),\n",
       " (u'uhs', 0.6368834972381592),\n",
       " (u'sdxc', 0.6269737482070923),\n",
       " (u'cf', 0.6235103607177734),\n",
       " (u'rw', 0.6204118132591248),\n",
       " (u'verbatim', 0.6137969493865967),\n",
       " (u'chipset', 0.5992798805236816)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and test trained model\n",
    "model = word2vec.Word2Vec.load(\"300features_10minwords_5context\")\n",
    "model.most_similar('lenovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'summer', 0.7962464094161987),\n",
       " (u'nights', 0.7220483422279358),\n",
       " (u'rainy', 0.6389271020889282),\n",
       " (u'fall', 0.6318002939224243),\n",
       " (u'weekend', 0.6239671111106873),\n",
       " (u'rides', 0.6066479682922363),\n",
       " (u'lounge', 0.6060628294944763),\n",
       " (u'layering', 0.6030182242393494),\n",
       " (u'season', 0.6007533073425293),\n",
       " (u'formal', 0.5823827385902405)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('winter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'blue', 0.8068062663078308),\n",
       " (u'white', 0.7729544043540955),\n",
       " (u'yellow', 0.7679733633995056),\n",
       " (u'gray', 0.7630442380905151),\n",
       " (u'purple', 0.7610870599746704),\n",
       " (u'pink', 0.7597286105155945),\n",
       " (u'navy', 0.7231534719467163),\n",
       " (u'fuchsia', 0.6991884708404541),\n",
       " (u'black', 0.6961089372634888),\n",
       " (u'tan', 0.6957448124885559)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
