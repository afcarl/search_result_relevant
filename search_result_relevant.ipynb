{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from unidecode import unidecode\n",
    "from bs4 import BeautifulSoup\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "train = pd.read_csv('data/train.csv').fillna(\"\")\n",
    "test = pd.read_csv('data/test.csv').fillna(\"\")\n",
    "\n",
    "# we dont need ID columns\n",
    "idx = test.id.values.astype(int)\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create labels. drop useless columns\n",
    "y = train.median_relevance.values\n",
    "train = train.drop(['median_relevance', 'relevance_variance'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_stuff(text_in):\n",
    "    \"\"\"\n",
    "    Regular expression to remove stuff that I can think of\n",
    "    \"\"\"\n",
    "    text_out = unidecode(BeautifulSoup(text_in).get_text())\n",
    "    text_out = re.sub('-', ' ', text_out)\n",
    "    text_out = re.sub('\\n', ' ', text_out)\n",
    "    text_out = re.sub(',', ' ', text_out)\n",
    "    text_out = re.sub('\\t', ' ', text_out)\n",
    "    text_out = re.sub('/', ' ', text_out)\n",
    "    text_out = re.sub(':', ' ', text_out)\n",
    "    text_out = re.sub('\\.', ' ', text_out)\n",
    "    text_out = re.sub('\\(', ' ', text_out)\n",
    "    text_out = re.sub('\\)', ' ', text_out)\n",
    "    text_out = re.sub('\"', \" \", text_out)\n",
    "    text_out = re.sub(\"'\", \" \", text_out)\n",
    "    text_out = re.sub(r'\\(.*?\\)', '',text_out) # remove remnant stuff in parenthesis i.e. (tm), (r) - we won't remove actually...\n",
    "    return text_out.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['product_title'] = train['product_title'].apply(remove_stuff)\n",
    "train['product_description'] = train['product_description'].apply(remove_stuff)\n",
    "test['product_title'] = test['product_title'].apply(remove_stuff)\n",
    "test['product_description'] = test['product_description'].apply(remove_stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import stem\n",
    "porter = stem.PorterStemmer() # porter stemmer\n",
    "\n",
    "def stem_string(text_in):\n",
    "    text_out = ' '.join([porter.stem(i.strip()) for i in text_in.split(' ')])\n",
    "    text_out = re.sub(r'\\([^)]*\\)', '', text_out) # remove stuff in parenthesis i.e. (tm), (r)\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stem string\n",
    "train['query_stem'] = train['query'].apply(stem_string)\n",
    "train['product_title_stem'] = train['product_title'].apply(stem_string)\n",
    "train['product_description_stem'] = train['product_description'].apply(stem_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test['query_stem'] = test['query'].apply(stem_string)\n",
    "test['product_title_stem'] = test['product_title'].apply(stem_string)\n",
    "test['product_description_stem'] = test['product_description'].apply(stem_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# do some lambda magic on text columns\n",
    "traindata = list(train.apply(lambda x:'%s %s %s' % (x['query_stem'],x['product_title_stem'], x['product_description_stem']),axis=1))\n",
    "testdata = list(test.apply(lambda x:'%s %s %s' % (x['query_stem'],x['product_title_stem'], x['product_description_stem']),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the infamous tfidf vectorizer (Do you remember this one?)\n",
    "tfv = TfidfVectorizer(min_df=3, max_features=20000, \n",
    "        strip_accents='unicode', analyzer='word',\n",
    "        ngram_range=(1, 2), use_idf=1, smooth_idf=1,sublinear_tf=True,\n",
    "        stop_words='english') # token_pattern=r'\\w{1,}',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfv.fit(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X =  tfv.transform(traindata)\n",
    "X_test = tfv.transform(testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add feature column to our prepared matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is how we get first rank :) basically if every search terms occur in the product title (or description), I will append 1. Rather than that, I will append 0. Actually it's not that good since we can have partly text occur and people give high score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see if query occur in title or not\n",
    "occurence_train = []\n",
    "for j in range(len(train)):\n",
    "    count = 0\n",
    "    for query_text in train.iloc[j]['query_stem'].split(' '):\n",
    "        if query_text in train.iloc[j]['product_title_stem']:\n",
    "            count += 1\n",
    "    if count == len(train.iloc[j]['query_stem'].split(' ')):\n",
    "        occurence_train.append(1)\n",
    "    else:\n",
    "        occurence_train.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see if query occur in description or not\n",
    "occurence_des_train = []\n",
    "for j in range(len(train)):\n",
    "    count = 0\n",
    "    for query_text in train.iloc[j]['query_stem'].split(' '):\n",
    "        if query_text in train.iloc[j]['product_description_stem']:\n",
    "            count += 1\n",
    "    if count == len(train.iloc[j]['query_stem'].split(' ')) or (train.iloc[j]['product_description_stem'] == '' and occurence_train == 1):\n",
    "        occurence_des_train.append(1)\n",
    "    else:\n",
    "        occurence_des_train.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see if query occur in title or not\n",
    "occurence_test = []\n",
    "for j in range(len(test)):\n",
    "    count = 0\n",
    "    for query_text in test.iloc[j]['query_stem'].split(' '):\n",
    "        if query_text in test.iloc[j]['product_title_stem']:\n",
    "            count += 1\n",
    "    if count == len(test.iloc[j]['query_stem'].split(' ')):\n",
    "        occurence_test.append(1)\n",
    "    else:\n",
    "        occurence_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see if query occur in description or not\n",
    "occurence_des_test = []\n",
    "for j in range(len(test)):\n",
    "    count = 0\n",
    "    for query_text in test.iloc[j]['query_stem'].split(' '):\n",
    "        if query_text in test.iloc[j]['product_description_stem']:\n",
    "            count += 1\n",
    "    if count == len(test.iloc[j]['query_stem'].split(' '))  or (test.iloc[j]['product_description_stem'] == '' and occurence_test == 1):\n",
    "        occurence_des_test.append(1)\n",
    "    else:\n",
    "        occurence_des_test.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word length ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see if query occur in title or not\n",
    "occurence_ratio_train = []\n",
    "for j in range(len(train)):\n",
    "    count = 0\n",
    "    m = float(len(train.iloc[j]['query_stem'].split(' ')))\n",
    "    for query_text in train.iloc[j]['query_stem'].split(' '):\n",
    "        if query_text.strip() in train.iloc[j]['product_title_stem']:\n",
    "            count += 1\n",
    "    occurence_ratio_train.append(count/m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see if query occur in title or not\n",
    "occurence_ratio_test = []\n",
    "for j in range(len(test)):\n",
    "    count = 0\n",
    "    m = float(len(test.iloc[j]['query_stem'].split(' ')))\n",
    "    for query_text in test.iloc[j]['query_stem'].split(' '):\n",
    "        if query_text.strip() in test.iloc[j]['product_title_stem']:\n",
    "            count += 1\n",
    "    occurence_ratio_test.append(count/m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X can be either from text with stemmer or without stemmer\n",
    "X_occur = np.hstack((X.todense(), np.atleast_2d(np.array(occurence_ratio_train)).T, np.atleast_2d(np.array(occurence_des_train)).T))\n",
    "X_occur = np.hstack((X_occur, np.atleast_2d(np.array(occurence_train)).T))\n",
    "X_occur = sparse.csr_matrix(X_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_test can either from text with stemmer or without stemmer\n",
    "X_test_occur = np.hstack((X_test.todense(), np.atleast_2d(np.array(occurence_ratio_test)).T, np.atleast_2d(np.array(occurence_des_test)).T))\n",
    "X_test_occur = np.hstack((X_test_occur, np.atleast_2d(np.array(occurence_test)).T))\n",
    "X_test_occur = sparse.csr_matrix(X_test_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# or try with simple Logistic regression\n",
    "model = LogisticRegression(penalty='l2', dual=True, tol=0.0001,\n",
    "                           C=5.0, fit_intercept=True, intercept_scaling=1.0,\n",
    "                           class_weight='auto', random_state=42)\n",
    "\n",
    "\n",
    "# Fit Logistic Regression Model\n",
    "model.fit(X_occur, y)\n",
    "preds = model.predict(X_test_occur)\n",
    "\n",
    "\n",
    "# Create your first submission file\n",
    "submission = pd.DataFrame({\"id\": idx, \"prediction\": preds})\n",
    "submission.to_csv(\"tf_idf_stem_occur_logistic_tuned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is how I use C=5.0 in logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import cross_validation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#C_list = [0.1, 0.3, 1, 3, 10, 100, 1000]\n",
    "C_list = [0.1, 0.3, 1, 3, 5, 6, 7, 10]\n",
    "\n",
    "results = []\n",
    "for C in C_list:\n",
    "    print C\n",
    "    model = LogisticRegression(penalty='l2', dual=True, tol=0.0001,\n",
    "                               C=C, fit_intercept=True, intercept_scaling=1.0,\n",
    "                               class_weight='auto', random_state=42)\n",
    "    scores = cross_validation.cross_val_score(model, X_occur, y, cv=5)\n",
    "    results.append([C, np.mean(scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = np.array(results)\n",
    "plt.scatter(results[:,0], results[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_occur[1][:-3].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "clf_2 = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "\n",
    "clf_2.fit(X_occur, y)\n",
    "preds_svm = clf_2.predict(X_test_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(preds_svm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.sum(preds != preds_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(X_occur[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_occur, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'penalty': ['l2'], 'dual': [True, False],\n",
    "                     'class_weight':['auto', None],\n",
    "                     'C': [0.1, 0.5, 5.5, 5., 6., 7.]},\n",
    "                    {'penalty': ['l1', 'l2'], 'dual': [False],\n",
    "                     'class_weight':['auto', None],\n",
    "                     'C': [0.1, 0.5, 5.5, 5., 6., 7.]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(LogisticRegression(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_weighted' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_occur, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'n_estimators': [10], \n",
    "                     'max_samples': [0.3, 0.5, 0.8, 1.0],\n",
    "                    'max_features': [0.3, 0.5, 0.8, 1.0]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(BaggingClassifier(n_jobs=8), tuned_parameters, cv=5,\n",
    "                       scoring='%s_weighted' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_occur, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'n_estimators': [ 5,10,20], \n",
    "                      'class_weight':[\"auto\", \"subsample\", None]}]\n",
    "#                     {'n_estimators': [ 5, 10, 20]\n",
    "#                      , 'class_weight':[\"auto\", \"subsample\", None]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(RandomForestClassifier(n_jobs=8), tuned_parameters, cv=5,\n",
    "                       scoring='%s_weighted' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
