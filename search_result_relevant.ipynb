{
 "metadata": {
  "name": "",
  "signature": "sha256:8db32790368a05cf042cb1b57a02f5fc3b6bf82c8ddc5e887733b6bfefd2fcd2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load and Preprocess data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from unidecode import unidecode\n",
      "from bs4 import BeautifulSoup\n",
      "import scipy\n",
      "from scipy import sparse\n",
      "\n",
      "train = pd.read_csv('train.csv').fillna(\"\")\n",
      "test = pd.read_csv('test.csv').fillna(\"\")\n",
      "\n",
      "# we dont need ID columns\n",
      "idx = test.id.values.astype(int)\n",
      "train = train.drop('id', axis=1)\n",
      "test = test.drop('id', axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create labels. drop useless columns\n",
      "y = train.median_relevance.values\n",
      "train = train.drop(['median_relevance', 'relevance_variance'], axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_stuff(text_in):\n",
      "    \"\"\"\n",
      "    Regular expression to remove stuff that I can think of\n",
      "    \"\"\"\n",
      "    text_out = unidecode(BeautifulSoup(text_in).get_text())\n",
      "    text_out = re.sub('-', ' ', text_out)\n",
      "    text_out = re.sub('\\n', ' ', text_out)\n",
      "    text_out = re.sub(',', ' ', text_out)\n",
      "    text_out = re.sub('\\t', ' ', text_out)\n",
      "    text_out = re.sub('/', ' ', text_out)\n",
      "    text_out = re.sub(':', ' ', text_out)\n",
      "    text_out = re.sub('\\.', ' ', text_out)\n",
      "    text_out = re.sub('\\(', ' ', text_out)\n",
      "    text_out = re.sub('\\)', ' ', text_out)\n",
      "    text_out = re.sub('\"', \" \", text_out)\n",
      "    text_out = re.sub(\"'\", \" \", text_out)\n",
      "    text_out = re.sub(r'\\(.*?\\)', '',text_out) # remove remnant stuff in parenthesis i.e. (tm), (r) - we won't remove actually...\n",
      "    return text_out.lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train['product_title'] = train['product_title'].apply(remove_stuff)\n",
      "train['product_description'] = train['product_description'].apply(remove_stuff)\n",
      "test['product_title'] = test['product_title'].apply(remove_stuff)\n",
      "test['product_description'] = test['product_description'].apply(remove_stuff)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import stem\n",
      "porter = stem.PorterStemmer() # porter stemmer\n",
      "\n",
      "def stem_string(text_in):\n",
      "    text_out = ' '.join([porter.stem(i.strip()) for i in text_in.split(' ')])\n",
      "    text_out = re.sub(r'\\([^)]*\\)', '', text_out) # remove stuff in parenthesis i.e. (tm), (r)\n",
      "    return text_out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stem string\n",
      "train['query_stem'] = train['query'].apply(stem_string)\n",
      "train['product_title_stem'] = train['product_title'].apply(stem_string)\n",
      "train['product_description_stem'] = train['product_description'].apply(stem_string)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test['query_stem'] = test['query'].apply(stem_string)\n",
      "test['product_title_stem'] = test['product_title'].apply(stem_string)\n",
      "test['product_description_stem'] = test['product_description'].apply(stem_string)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Prepare Matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# do some lambda magic on text columns\n",
      "traindata = list(train.apply(lambda x:'%s %s %s' % (x['query'],x['product_title'], x['product_description']),axis=1))\n",
      "testdata = list(test.apply(lambda x:'%s %s %s' % (x['query'],x['product_title'], x['product_description']),axis=1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the infamous tfidf vectorizer (Do you remember this one?)\n",
      "tfv = TfidfVectorizer(min_df=3, max_features=None, \n",
      "        strip_accents='unicode', analyzer='word',\n",
      "        ngram_range=(1, 2), use_idf=1, smooth_idf=1,sublinear_tf=1,\n",
      "        stop_words='english') # token_pattern=r'\\w{1,}',"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfv.fit(traindata)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X =  tfv.transform(traindata)\n",
      "X_test = tfv.transform(testdata)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Add feature column to our prepared matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "this is how we get first rank :) basically if every search terms occur in the product title (or description), I will append 1. Rather than that, I will append 0. Actually it's not that good since we can have partly text occur and people give high score"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# see if query occur in title or not\n",
      "occurence_train = []\n",
      "for j in range(len(train)):\n",
      "    count = 0\n",
      "    for query_text in train.iloc[j]['query_stem'].split(' '):\n",
      "        if query_text in train.iloc[j]['product_title_stem']:\n",
      "            count += 1\n",
      "    if count == len(train.iloc[j]['query_stem'].split(' ')):\n",
      "        occurence_train.append(1)\n",
      "    else:\n",
      "        occurence_train.append(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# see if query occur in description or not\n",
      "occurence_des_train = []\n",
      "for j in range(len(train)):\n",
      "    count = 0\n",
      "    for query_text in train.iloc[j]['query_stem'].split(' '):\n",
      "        if query_text in train.iloc[j]['product_description_stem']:\n",
      "            count += 1\n",
      "    if count == len(train.iloc[j]['query_stem'].split(' ')) or (train.iloc[j]['product_description_stem'] == '' and occurence_train == 1):\n",
      "        occurence_des_train.append(1)\n",
      "    else:\n",
      "        occurence_des_train.append(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# see if query occur in title or not\n",
      "occurence_test = []\n",
      "for j in range(len(test)):\n",
      "    count = 0\n",
      "    for query_text in test.iloc[j]['query_stem'].split(' '):\n",
      "        if query_text in test.iloc[j]['product_title_stem']:\n",
      "            count += 1\n",
      "    if count == len(test.iloc[j]['query_stem'].split(' ')) or (test.iloc[j]['product_description_stem'] == '' and occurence_test == 1):\n",
      "        occurence_test.append(1)\n",
      "    else:\n",
      "        occurence_test.append(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# see if query occur in description or not\n",
      "occurence_des_test = []\n",
      "for j in range(len(test)):\n",
      "    count = 0\n",
      "    for query_text in test.iloc[j]['query_stem'].split(' '):\n",
      "        if query_text in test.iloc[j]['product_description_stem']:\n",
      "            count += 1\n",
      "    if count == len(test.iloc[j]['query_stem'].split(' ')):\n",
      "        occurence_des_test.append(1)\n",
      "    else:\n",
      "        occurence_des_test.append(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# X can be either from text with stemmer or without stemmer\n",
      "X_occur = sparse.csr_matrix(np.hstack((X.todense(), np.atleast_2d(np.array(occurence_train)).T, np.atleast_2d(np.array(occurence_des_train)).T)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# X_test can either from text with stemmer or without stemmer\n",
      "X_test_occur = sparse.csr_matrix(np.hstack((X_test.todense(), np.atleast_2d(np.array(occurence_test)).T, np.atleast_2d(np.array(occurence_des_test)).T)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# or try with simple Logistic regression\n",
      "model = LogisticRegression(penalty='l2', dual=True, tol=0.0001,\n",
      "                           C=5.0, fit_intercept=True, intercept_scaling=1.0,\n",
      "                           class_weight='auto', random_state=42)\n",
      "\n",
      "\n",
      "# Fit Logistic Regression Model\n",
      "model.fit(X_occur, y)\n",
      "preds = model.predict(X_test_occur)\n",
      "\n",
      "\n",
      "# Create your first submission file\n",
      "submission = pd.DataFrame({\"id\": idx, \"prediction\": preds})\n",
      "submission.to_csv(\"/home/ubuntu/search_result_relevance/tf_idf_stem_occur_logistic.csv\", index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "This is how I use C=5.0 in logistic regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from sklearn import cross_validation\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#C_list = [0.1, 0.3, 1, 3, 10, 100, 1000]\n",
      "C_list = [0.1, 0.3, 1, 3, 5, 6, 7, 10]\n",
      "\n",
      "results = []\n",
      "for C in C_list:\n",
      "    print C\n",
      "    model = LogisticRegression(penalty='l2', dual=True, tol=0.0001,\n",
      "                               C=C, fit_intercept=True, intercept_scaling=1.0,\n",
      "                               class_weight='auto', random_state=42)\n",
      "    scores = cross_validation.cross_val_score(model, X_occur, y, cv=5)\n",
      "    results.append([C, np.mean(scores)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = np.array(results)\n",
      "plt.scatter(results[:,0], results[:,1])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}